---
title: "Introduction to SC19087"
author: "19087"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to SC19087}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__SC19087__ is a simple R package developed to write two functions and record the homework. Two functions are peBS, which is used to estimate BS density, and rBS, which is used to generate random sample from BS destiny. 

The rest is my homework.

---  

## Homework  

---  




##Question

Go through R for Beginners if you are not familiar with Rprogramming.

Use knitr to produce at least 3 examples (texts, figures,tables).

----


##Example 1

```{r}
data<-rnorm(100,mean=0,sd=1)
shapiro.test(data)
```
The p-value is `r shapiro.test(data)$p`.

----

##Example 2


```{r}
library(vcd)
knitr::kable(head(Arthritis))
```


----

##Example 3

```{r}
qqnorm(data)
```
  
---  



## Question 3.4  

The Rayleigh density [156, Ch. 18] is  
$$f(x)=\frac{x}{\sigma^2}e^{-x^2/(2\sigma^2)}, x??0,\sigma>0$$ 
Develop an algorithm to generate random samples from a Rayleigh($\sigma$) distribution. Generate Rayleigh($\sigma$) samples for several choices of \sigma > 0 and check that the mode of the generated samples is close to the theoretical mode $\sigma$ (check the histogram).  

---  

## The answer to question 3.4.  

We use general algorithm.  
Easily,from $f(x)$,we can find that $$F_{X}(x)=1-e^{-x^2/(2\sigma^2)}.$$  
Hence we can generate random numbers U from U[0,1],and $$X=F_{X}^{-1}(U).$$  
Where $F_{X}^{-1}(U)=\sqrt{log(1-U)*(-2\sigma^2)}$
```{r}  

Rayleigh<-function(sigma)
{ U<-runif(10000)
  X<-1:10000
  for(i in 1:10000){
  X[i]<-sqrt((-2*(sigma)^2)*log(1-U[i]))}
  X
}  
checkfunc<-function(sigma){
  x<-Rayleigh(sigma)
  t=1/(sigma*sqrt(exp(1)))
  hist(x,prob=TRUE,main="check the pdf",ylim=c(0,t))
  y<-seq(0,5,.01)
  lines(y,y*exp(-y^2/(2*sigma^2))/(sigma)^2)
}
```    
NOTE:the top of the histogram equals to f($\sigma$)(the maximum of f(x)).  
for $\sigma$=1  
the head of random numbers are   
```{r} 
head(Rayleigh(1))
```  
and the histogram for check is:  
```{r} 
checkfunc(1)
```  

for $\sigma$=0.5    
the head of random numbers are  
```{r} 
head(Rayleigh(0.5))
```
and the histogram for check is:  
```{r} 
checkfunc(0.5)  
```  

for $\sigma$=0.75    
the head of random numbers are   
```{r}
head(Rayleigh(0.75))
```  
and the histogram for check is:  
```{r} 
checkfunc(0.75)  
```  

for $\sigma$=0.25    
the head of random numbers are   
```{r}
head(Rayleigh(0.25))
```  
and the histogram for check is:  
```{r} 
checkfunc(0.25)  
```  

Hence,the mode seems to fit well.   

---  

##Question 3.11  

 Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have N(0, 1) and N(3, 1) distributions with mixing probabilities p1 and p2 = 1-p1. Graph the histogram of the sample with density superimposed, for p1 = 0.75. Repeat with different values for p1 and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of p1 that produce bimodal mixtures.  

---  

## The answer to question 3.11  

First we write the function to generate the random sample of size 1000 and graph the histogram for unknown p1.  

```{r}
normalmix<-function(p){
  U<-runif(1000)
  X<-1:1000
  for(i in 1:1000){
    X[i]=(U[i]<=p)*rnorm(n=1,mean=0,sd=1)+(U[i]>p)*rnorm(n=1,mean=3,sd=1)
  }
  X
}
refunc<-function(p){
  x<-normalmix(p)
  cat("The head of sample for p1=",p,"is:",head(x),"\n")
  hist(x,prob=TRUE,ylim=c(0,0.8))
  y<-seq(-10,10,0.01)
  lines(y,p*dnorm(x=y,mean=0,sd=1)+(1-p)*dnorm(x=y,mean=3,sd=1))
}
```  

for p1=0.75  
```{r}
refunc(0.75)
```  

We choose p1=0.25,0.5 first:  
```{r}
refunc(0.25)
```  

```{r}
refunc(0.5)
```   


We find that when p1 is close to 0.5, it has two similar peaks; as p1 stays away from 0.5,one peak seems to be small and small.So we guess the values of p1 that produce bimodal mixtures are nearly in [0.25,0.75].  
We choose p1=0.2,0.3,0.7,0.8 then:  

```{r}
refunc(0.2)
```  

```{r}
refunc(0.3)
```   

```{r}
refunc(0.7)
```   

```{r}
refunc(0.8)
```   
  
  We find that one peak are too small to observe,so we can't give a more accurate conjecture than [0.25,0.75] by observation.By calculation, we find the limits of rate p1/p2 are 4.1755858 and 0.2394874,in other words, p1 is 0.8067852 and 0.1932148.  
  

---  

## Question 3.18  

Write a function to generate a random sample from a $W_d(\Sigma,n)$ (Wishart) distribution for n>d+1??1, based on Bartlett's decomposition.  

---  

## The answer to question 3.18  

For example ,we assume d=3,n=5,and  
$$\Sigma=
\left\{
\begin{matrix}
1 & 0.5 & 0.5 \\
0.5 & 1 & 0.5 \\
0.5 & 0.5 & 1 
\end{matrix}
\right\}$$


Then we find the algorithm to generate random samples. 

```{r}  

wishart<-function(n) #n:sample size
{
  arr<-array(1:(9*n),dim=c(3,3,n))  #save the T
  matrix<-array(1:(9*n),dim=c(3,3,n))  #save the LAL^T
  t<-rnorm(3*n) #tij:i>j
  s1<-rchisq(n,df=5)
  s2<-rchisq(n,df=4)
  s3<-rchisq(n,df=3)
  t1<-sqrt(s1) #t11
  t2<-sqrt(s2) #t22
  t3<-sqrt(s3) #t33
  S<-matrix(c(1,0.5,0.5,0.5,1,0.5,0.5,0.5,1),ncol=3)
  l<-chol(S)  #matrix l is a upper triangular,t(l) is what we need
  for(i in 1:n){
    arr[,,i]<-matrix(c(t1[i],t[3*i-2],t[3*i-1],0,t2[i],t[3*i],0,0,t3[i]),ncol=3)
    matrix[,,i]<-t(l)%*%arr[,,i]%*%t(arr[,,i])%*%l
  }
  matrix
}

```

For example, we want to generate a sample of size 10, we can:  

```{r}

wishart(10)

```   

---  

## Question 5.1  

Compute a Monte Carlo estimate of  $$ \displaystyle \int ^{\pi/3}_{0}{sint dt}$$  
and compare your estimate with the exact value of the integral.  

---  

## Answer  

We use the simple algorithm.  
$$g(t)=\frac{\pi}{3}sint,T \sim  U(0,\frac{pi}{3})$$  
Then we generate $T_1,T_2,...,T_m$ from T and calculate $\frac{1}{m}\sum_{i=1}^{m}g(T_i).$  

```{r}

m<-1e4
T<-runif(m,min=0,max=pi/3)  
theta.hat<-mean(sin(T)*pi/3)  
print(c(theta.hat,-cos(pi/3)+cos(0)))  
```

The bias is small.  

---  

## Question 5.10  

Use Monte Carlo integration with antithetic variables to estimate  
$$\displaystyle \int ^{1}_{0} {\frac{e^{-x}}{1+x^2}dx}$$  
and find the approximate reduction in variance as a percentage of the variance without variance reduction.  

---  

## Answer  

```{r}
m<-1e4
MC1<-1:1000
MC2<-1:1000
set.seed(123)
for(i in 1:1000){

u<-runif(m/2)
v<-runif(m/2)
x1<-c(u,1-u)       ##sample with antithetic reduction
x2<-c(u,v) ##sample without variance reduction
MC1[i]<-mean(exp(-x1)/(1+x1^2))
MC2[i]<-mean(exp(-x2)/(1+x2^2))
}
print(c(mean(MC1),mean(MC2),1-var(MC1)/var(MC2)))
```
The estimate with antithetic variables is `r mean(MC1)`.  
The percentage is about `r 1-var(MC1)/var(MC2)` .  

---  

## Question 5.15  


Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.  

---  

## Answer  
for interval (0,1)
$$F(x)=\frac{1-e^{-x}}{1-e^{-1}}$$
$$F^{-1}(x)=-log(1-(1-e^{-1})x)$$

So we divide the interval (0,1) into five subintervals:$(0,F^{-1}(0.2)),(F^{-1}(0.2),F^{-1}(0.4)),(F^{-1}(0.4),F^{-1}(0.6)),(F^{-1}(0.6),F^{-1}(0.8)),(F^{-1}(0.8),F^{-1}(1)).$  


for subintervals:

Note $F_0(x)$ as the cdf of x.
So $F_0(x)=5 \cdot \frac{e^{-qi}-e^{-x}}{1-e^{-1}},the \space ith\space subinterval$  
$$F_0^{-1}(x)=-log(e^{-qi}-(1-e^{-1})\frac{x}{5}),the \space ith\space subinterval$$  
for the ith subinterval, qi is the inf of the subinterval.
```{r}
a<-c(0.2,0.4,0.6,0.8)
p<--log(1-(1-exp(-1))*a)
u1<-runif(2000)
u2<-runif(2000)
u3<-runif(2000)
u4<-runif(2000)
u5<-runif(2000)
x1<--log(1-(1-exp(-1))*u1/5)     ## xi: the random sample in ith subinterval
x2<--log(exp(-p[1])-(1-exp(-1))*u2/5)
x3<--log(exp(-p[2])-(1-exp(-1))*u3/5)
x4<--log(exp(-p[3])-(1-exp(-1))*u4/5)
x5<--log(exp(-p[4])-(1-exp(-1))*u5/5)
y1<-exp(-x1)/(1+x1^2)/(5*exp(-x1)/(1-exp(-1)))
y2<-exp(-x2)/(1+x2^2)/(5*exp(-x2)/(1-exp(-1)))
y3<-exp(-x3)/(1+x3^2)/(5*exp(-x3)/(1-exp(-1)))
y4<-exp(-x4)/(1+x4^2)/(5*exp(-x4)/(1-exp(-1)))
y5<-exp(-x5)/(1+x5^2)/(5*exp(-x5)/(1-exp(-1)))
fg<-y1+y2+y3+y4+y5
mean(fg)
sd(fg)
```  

By calculation, this estimate is more accurate and has smaller standard error.  

---  

## Question 6.5  

  Suppose a 95% symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size n = 20. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.)  
 
 
---  

## Answer  

```{r}
n<-20
alpha<-0.05
m=1e3
UCL1<-numeric(m)
LCL1<-numeric(m)
judge1<-numeric(m)
UCL2<-numeric(m)
LCL2<-numeric(m)
judge2<-numeric(m)
for(i in 1:m){
x<-rchisq(n,df=2)
UCL1[i]<-mean(x)+qt(1-alpha/2,n-2)*sd(x)/sqrt(n)
LCL1[i]<-mean(x)-qt(1-alpha/2,n-2)*sd(x)/sqrt(n)
UCL2[i]<-(n-1)*var(x)/qchisq(alpha,df=n-1) 
judge1[i]<-(UCL1[i]>2&&LCL1[i]<2)+0  ##t-interval
judge2[i]<-(UCL2[i]>4)+0  ##interval for variance
}  ## for chisq distribution with df=n,the mean of r.v. is n,the var is 2n.X~chisq(2),so mean(X)=2,var(X)=4

```  

The estimate of coverage probability of t-interval is 
```{r} 
mean(judge1)
```

The estimate of coverage probability of interval for variance is 
```{r} 
mean(judge2)
```

It's far greater than the interval for variance,as the question mentioned, 'the t-interval should be more robust to departures from normality than the interval for variance'.  

---  

##Question 6.6  

  Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness $\sqrt{b1}$ under normality by a Monte Carlo experiment. Compute the standard error of the estimates from (2.14) using the normal approximation for the density (with exact variance formula). Compare the estimated quantiles with the quantiles of the large sample approximation $\sqrt{b1}$ ?? N(0,6/n).  
 
---  

## Answer  

First, we estimate the quantiles.

```{r}
n<-1e3
m<-1e4
sk<-numeric(n)
for(i in 1:n){
  x<-rnorm(m)  ## generate samples from N(0,1)
  xbar<-mean(x)
  m3<-mean((x-xbar)^3)
  m2<-mean((x-xbar)^2)
  sk[i]<-m3/m2^1.5
}
sk<-sort(sk)
xq<-sk[c(floor(0.025*n),floor(0.05*n),ceiling(0.95*n),ceiling(0.975*n))]
print(xq)##the estimated quantiles  
```  

Then we compute the standard error.  

```{r}
q<-c(0.025,0.05,0.95,0.975)
var_xq<-q*(1-q)/(n*dnorm(xq,mean=0,sd=sqrt(6*(n-2)/(n+1)/(n+3)))^2)
sd_xq<-sqrt(var_xq)
print(sd_xq)
```  

Then we compare the estimated quantiles with the quantiles of N(0,6/n).  

```{r}
print(xq)  
print(qnorm(c(0.025,0.05,0.95,0.975),mean=0,sd=sqrt(6/n)))
```  

We find they are not similar, so we take a larger n just for comparison.  

```{r}

n<-1e4
m<-1e4
sk<-numeric(n)
for(i in 1:n){
  x<-rnorm(m)  ## generate samples from N(0,1)
  xbar<-mean(x)
  m3<-mean((x-xbar)^3)
  m2<-mean((x-xbar)^2)
  sk[i]<-m3/m2^1.5
}
sk<-sort(sk)
xq<-sk[c(floor(0.025*n),floor(0.05*n),ceiling(0.95*n),ceiling(0.975*n))]
print(xq)
print(qnorm(c(0.025,0.05,0.95,0.975),mean=0,sd=sqrt(6/n)))
```  

This time we get a much better conclusion. And we see the estimated quantiles don't have a big change,which means the estimate is great.  

And for n=10000,the sds of quantiles are:  

```{r}
q<-c(0.025,0.05,0.95,0.975)
var_xq<-q*(1-q)/(n*dnorm(xq,mean=0,sd=sqrt(6*(n-2)/(n+1)/(n+3)))^2)
sd_xq<-sqrt(var_xq)
print(sd_xq)
```  

It's much smaller than n=1000.  

---  

## Question 6.7  

 Estimate the power of the skewness test of normality against symmetric Beta($\alpha,\alpha$) distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as t($\nu$)?  
 
---  

## Answer  

```{r}
sk <- function(x) {
#computes the sample skewness coeff.
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}
alpha<-0.1
n<-30
m<-2500
cv<-qnorm(1-alpha/2,0,sqrt(6*(n-2)/((n+1)*(n+3))))
sktests1<-sktests2<-numeric(m)
for(i in 1:m){
  x1<-rbeta(n,alpha,alpha)
  x2<-rt(n,df=2)
  sktests1[i]<-as.integer(abs(sk(x1)>=cv))
  sktests2[i]<-as.integer(abs(sk(x2)>=cv))
}
pwj1<-mean(sktests1)
pwj2<-mean(sktests2)
print(c(pwj1,pwj2))

```
For beta distribution the power is very small.But for heavy-tailed distribution it's very big.

---  

## Question 6.A  

Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level
$\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^2$(1), (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case, test H0 : $\mu$ = $\mu_0$ vs H0 : $\mu \neq \mu_0$, where $\mu_0$ is the mean of $\chi^2$(1), Uniform(0,2), and Exponential(1),respectively.  

---  

## Answer  

```{r}
n<-20
alpha=0.05
mu0<-1#these three distribution have the same mean

m<-10000
p1<-p2<-p3<-numeric(m)
for(j in 1:m){
  x1<-rchisq(n,df=1)##X~chisq(1)
  ttest1<-t.test(x1,alternative="two.sided",mu=mu0)
  p1[j]<-ttest1$p.value
  x2<-runif(n,min=0,max=2)##X~U(0,2)
  ttest2<-t.test(x2,alternative="two.sided",mu=mu0)
  p2[j]<-ttest2$p.value
  x3<-rexp(n,rate=1)##X~exp(1)
  ttest3<-t.test(x3,alternative="two.sided",mu=mu0)
  p3[j]<-ttest3$p.value
}
p1.hat<-mean(p1<alpha)
p2.hat<-mean(p2<alpha)
p3.hat<-mean(p3<alpha)
print(c(p1.hat,p2.hat,p3.hat))
```  
For the sample (i) and (iii), type I error > $\alpha$.  
For the sample (ii),type I error is very close to $\alpha$.  
In fact if we choose a bigger n (for example, 200),then both these three numbers will be very close to $\alpha$.

---  

## Question on slides  

![](C:\Users\Administrator\Pictures\5.png)  

---  

## Answer  

(1)  
  The hypothesis test problem is whether the powers are the same, in other words, whether the True Negative Rates are the same, which have the hypothesis as:  
$H_0$: They have the same power. And $H_a$: They have different powers.  

(2)  
  We should use the McNemar test, it's a test usually used in paired four-cell table.  

(3)  
  We need to know the number of the experiments which are reject $H_0^{'}$ in fact,we note the number as n. And we need to know one of the a,b,c,d which are shown in the table. Here $H_0^{'}$ is the null hypothesis of the hypothesis problem which has 10000 experiments as the question described. Then for these experiments, it will be positive (accept $H_0^{'}$) or negative (reject $H_0^{'}$) in two methods. As we know power1 and power2,then we can get a complete paired four-cell table (by calculations) which includes four numbers: the number of experiments be postive in both methods,the number of experiments be postive in method 1 but be negative in method 2,the number of experiments be postive in method 2 but be negative in method 1 and the  number of experiments be negative in both methods.Then we can use McNemar test to this table.  


The paired four-cell table is a table like this:  

![](C:\Users\Administrator\Pictures\7.png)  

---  

## Question 7.6  

Efron and Tibshirani discuss the scor (bootstrap) test score data on 88 students who took examinations in five subjects [84, Table 7.1],[188,Table 1.2.1].The first two tests (mechanics, vectors) were closed book and the last three tests (algebra, analysis, statistics) were open book. Each row of the data frame is a set of scores ($x_{i1},...,x_{i5}$) for the ith student. Use a panel display to display the scatter plots for each pair of test scores. Compare the plot with the sample correlation matrix. Obtain bootstrap estimates of the standard errors for each of the following estimates: $\hat{\rho}_{12} = \hat{\rho}$(mec,vec), $\hat{\rho}_{34} = \hat{\rho}$(alg,ana), $\hat{\rho}_{35} = \hat{\rho}$(alg,sta), $\hat{\rho}_{45} = \hat{\rho}$(ana,sta).  

---  

## Answer  

The display:  

```{r}
library(bootstrap)
di<-function(x,y){
  plot(scor[,x],scor[,y],xlab=colnames(scor)[x],ylab=colnames(scor)[y])
}

for(i in 1:4){
  for(j in (i+1):5){
    di(i,j)
  }
}
```  

And we do the compare with the sample correlation matrix:  

```{r}
cor(scor)  
```  

We can find they fit well. The pair which has bigger correlation coefficient seems to be more linear in the plot and the pair which has smaller correlation coefficient seems to be more irregular.  

Then we do the bootstrap estimate:  

```{r}
B<-200
n<-nrow(scor)
rho12<-rho34<-rho35<-rho45<-numeric(B)
for(j in 1:B){
  i<-sample(1:n,size=n,replace=TRUE)# a sample
  mec<-scor$mec[i]          
  vec<-scor$vec[i]
  alg<-scor$alg[i]
  ana<-scor$ana[i]
  sta<-scor$sta[i]
  rho12[j]<-cor(mec,vec)   # the correlation coefficient of the sample
  rho34[j]<-cor(alg,ana)
  rho35[j]<-cor(alg,sta)
  rho45[j]<-cor(ana,sta)
}
print(c(sd(rho12),sd(rho34),sd(rho35),sd(rho45)))
```  

---  

## Question 7.B  

Repeat Project 7.A for the sample skewness statistic. Compare the coverage rates for normal populations (skewness 0) and $\chi^2$(5) distributions (positive skewness).  

---  

## Answer  

In this question,we use judge variables for three intervals in each population.If judge=0, it means miss on the left. If judge=2, it means miss on the right. If judge=1, it means coverage.  

After calculation, the skewness of chisq(5) is $\frac{4}{\sqrt{10}}$.


```{r}
n<-20
B<-200
N<-1e3
alpha<-0.05
sknormal<-skchisq<-numeric(B)# save sample skewness
judge11<-judge12<-judge13<-numeric(N)#three judge variables for normal population
judge21<-judge22<-judge23<-numeric(N)#three judge variables for chisq population
sk <- function(x) {
#computes the sample skewness coeff.
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}
for(i in 1:N){
  x<-rnorm(n)
  y<-rchisq(n,df=5)
  for(j in 1:B){
    b<-sample(1:n,size=n,replace=TRUE)
    samp1<-x[b]
    samp2<-y[b]
    sknormal[j]<-sk(samp1)
    skchisq[j]<-sk(samp2)
  }
  sknormal<-sort(sknormal)
  hat1<-mean(sknormal)
  se1<-sd(sknormal)
  skchisq<-sort(skchisq)
  hat2<-mean(skchisq)
  se2<-sd(skchisq)
  judge11[i]<-(0>hat1-qnorm(1-alpha/2)*se1)+(0>hat1+qnorm(1-alpha/2)*se1)+0
  judge12[i]<-(0>hat1*2-sknormal[B*(1-alpha/2)])+(0>hat1*2-sknormal[B*alpha/2])+0
  judge13[i]<-(0>sknormal[B*alpha/2])+(0>sknormal[B*(1-alpha/2)])
  judge21[i]<-((4/sqrt(10))>hat2-qnorm(1-alpha/2)*se2)+((4/sqrt(10))>hat2+qnorm(1-alpha/2)*se2)+0
  judge22[i]<-((4/sqrt(10))>hat2*2-skchisq[B*(1-alpha/2)])+((4/sqrt(10))>hat2*2-skchisq[B*alpha/2])+0
  judge23[i]<-((4/sqrt(10))>skchisq[B*alpha/2])+((4/sqrt(10))>skchisq[B*(1-alpha/2)]) 
}
##For normal population
##standard normal bootstrap confidence interval
cat("The coverage probability,the proportion of miss on the left,the proportion of miss on the right is",c(sum(judge11==1),sum(judge11==0),sum(judge11==2))/N)
##basic bootstrap confidence interval
cat("The coverage probability,the proportion of miss on the left,the proportion of miss on the right is",c(sum(judge12==1),sum(judge12==0),sum(judge12==2))/N)
##percentile confidence interval
cat("The coverage probability,the proportion of miss on the left,the proportion of miss on the right is",c(sum(judge13==1),sum(judge13==0),sum(judge13==2))/N)
##For chisq population
##standard normal bootstrap confidence interval
cat("The coverage probability,the proportion of miss on the left,the proportion of miss on the right is",c(sum(judge21==1),sum(judge21==0),sum(judge21==2))/N)
##basic bootstrap confidence interval
cat("The coverage probability,the proportion of miss on the left,the proportion of miss on the right is",c(sum(judge22==1),sum(judge22==0),sum(judge22==2))/N)
##percentile confidence interval
cat("The coverage probability,the proportion of miss on the left,the proportion of miss on the right is",c(sum(judge23==1),sum(judge23==0),sum(judge23==2))/N)
```  

We can see,for normal population,the coverage probability is much bigger than chisq population.


---  

## Question 7.8  

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\hat{\theta}$.  

---  

## Answer  


```{r}
library(bootstrap)
n<-nrow(scor)
lambda_hat <- eigen(cov(scor))$values
theta_hat <- lambda_hat[1] / sum(lambda_hat)
theta_j <- rep(0,n)
for (i in 1:n) {
x<-scor [-i,]
lambda<-eigen(cov(x))$values
theta_j[i]<-lambda[1]/sum(lambda)
}
bias_jack<-(n-1)*(mean(theta_j)-theta_hat)
# the estimated bias of theta_hat
se_jack<-(n-1)*sqrt(var(theta_j)/n)
# the estimated se of theta_hat
print(c(bias_jack,se_jack))
```  

---  

## Question 7.10  

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the cross validation procedure? Which model is selected according to maximum adjusted $R^2$?  

---  

## Answer  

By the cross validation procedure:

```{r}
library(DAAG)
attach(ironslag)
n<-length(magnetic)
e1<-e2<-e3<-e4<-numeric(n)
for(k in 1:n){
  y<-magnetic[-k]
  x<-chemical[-k]
  
  L1<-lm(y~x)
  yhat1<-L1$coef[1]+L1$coef[2]*chemical[k]
  e1[k]<-magnetic[k]-yhat1
  
  L2<-lm(y~x+I(x^2))
  yhat2<-L2$coef[1]+L2$coef[2]*chemical[k]+L2$coef[3]*chemical[k]^2
  e2[k]<-magnetic[k]-yhat2
  
  L3<-lm(log(y)~x)
  logyhat3<-L3$coef[1]+L3$coef[2]*chemical[k]
  yhat3<-exp(logyhat3)
  e3[k]<-magnetic[k]-yhat3
  
  L4<-lm(y~x+I(x^2)+I(x^3))
  yhat4<-L4$coef[1]+L4$coef[2]*chemical[k]+L4$coef[3]*chemical[k]^2+L4$coef[4]*chemical[k]^3
  e4[k]<-magnetic[k]-yhat4
  
}
c(mean(e1^2),mean(e2^2),mean(e3^2),mean(e4^2))
```  

So we choose the Model 2 by the cross validation procedure.  
The model is:  

```{r}
LM2<-lm(magnetic~chemical+I(chemical^2))
cat("Y=",LM2$coef[1],LM2$coef[2],"X","+",LM2$coef[3],"X^2")
```  

By maximum adjusted $R^2$:  

```{r}

LM1<-lm(magnetic~chemical)
LM2<-lm(magnetic~chemical+I(chemical^2))
LM3<-lm(log(magnetic)~chemical)
LM4<-lm(magnetic~chemical+I(chemical^2)+I(chemical^3))
c(summary(LM1)$adj.r.squared,summary(LM2)$adj.r.squared,summary(LM3)$adj.r.squared,summary(LM4)$adj.r.squared)
```  

So we choose the Model 2 by maximum adjusted $R^2$.  
The model is:  
```{r}

cat("Y=",LM2$coef[1],LM2$coef[2],"X","+",LM2$coef[3],"X^2")
```    

---  

## Question 8.3  

 The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.  
 
---  

## Answer  

We'll use the K-S statistic.  

```{r}
R<-999
n1<-20
n2<-30
mu1<-mu2<-0
sigma1<-sigma2<-1
x<-rnorm(n1,mean=mu1,sd=sigma1)
y<-rnorm(n2,mean=mu2,sd=sigma2)

z<-c(x,y)
K<-1:(n1+n2)
D<-numeric(R)
options(warn=-1)
D0<-ks.test(x,y,exact=FALSE)$statistic
for(i in 1:R){
  k<-sample(K,size=n1,replace=FALSE)
  x1<-z[k]
  y1<-z[-k]
  D[i]<-ks.test(x1,y1,exact=FALSE)$statistic
}
p<-mean(c(D0,D)>=D0)
options(warn=0)
print(p)
```  

The approximate ASL `r p` does not support the alternative hypothesis that distributions differ.   

A histogram of the replicates of D is displayed by:  

```{r}
hist(D, main = "", freq = FALSE, breaks = "scott")
points(D0, 0, cex = 1, pch = 16)  

```  

---  

## Question on slides  

Power comparison (distance correlation test versus ball covariance test)  
  
  Model 1: $Y=X/4+e$  
  Model 2: $Y=X/4\times e$  
  $X\sim N(0_2,I_2)$, $e\sim N(0_2,I_2)$, $X$ and $e$ are independent.  
  
---  

## Answer  

```{r}
library(Ball)
library(boot)
library(mvtnorm)
alpha<-0.1
n0<-c(10,20,30,40,50,60,70,80,90,100)
dCov <- function(x, y) {
x <- as.matrix(x)
y <- as.matrix(y)
n <- nrow(x)
m <- nrow(y)
if (n != m || n < 2) stop("Sample sizes must agree")
if (! (all(is.finite(c(x, y)))))
stop("Data contains missing or infinite values")
Akl <- function(x) {
d <- as.matrix(dist(x))
m <- rowMeans(d)
M <- mean(d)
a <- sweep(d, 1, m)
b <- sweep(a, 2, m)
return(b + M)
}
A <- Akl(x)
B <- Akl(y)
dCov <- sqrt(mean(A * B))
dCov
}
pd1<-pd2<-pb1<-pb2<-numeric(100)
pod1<-pod2<-pob1<-pob2<-numeric(10)
ndCov2 <- function(z, ix, dims) {
  #dims contains dimensions of x and y 
  p <- dims[1]
  q <- dims[2]
  d <- p + q
  x <- z[ , 1:p] #leave x as is
  y <- z[ix, -(1:p)] #permute rows of y 
  return(nrow(z) * dCov(x, y)^2)
}
for(i in 1:10){
  for(j in 1:100){
  I2<-matrix(c(1,0,0,1),nrow=2)
  x<-rmvnorm(n0[i],rep(0,2),I2)
  e<-rmvnorm(n0[i],rep(0,2),I2)
  y1<-x/4+e
 
  y2<-x/4*e
  z1<-cbind(x,y1)
  z2<-cbind(x,y2)
  boot.obj1<-boot(data=z1,statistic=ndCov2,R=99,sim="permutation",dims=c(2,2))
  boot.obj2<-boot(data=z2,statistic=ndCov2,R=99,sim="permutation",dims=c(2,2))
  tb1<-c(boot.obj1$t0,boot.obj1$t)
  tb2<-c(boot.obj2$t0,boot.obj2$t)
  pd1[j]<-mean(tb1>=tb1[1])
  pd2[j]<-mean(tb2>=tb2[1])
  pb1[j]<-bcov.test(z1[,1:2],z1[,3:4],R=100,seed=j*12345)$p.value
  pb2[j]<-bcov.test(z2[,1:2],z2[,3:4],R=100,seed=j*23456)$p.value
  }
  pod1[i]<-mean(pd1<alpha)
  pod2[i]<-mean(pd2<alpha)
  pob1[i]<-mean(pb1<alpha)
  pob2[i]<-mean(pb2<alpha)
}
## Model 1
plot(n0,pod1,type="l",xlab='sample size',ylab='power',ylim=c(0,1),main='Model 1')
lines(n0,pob1,type="l")
points(n0,pod1)
points(n0,pob1)
## Model 2
plot(n0,pod2,type="l",xlab='sample size',ylab='power',ylim=c(0,1),main='Model 2')
lines(n0,pob2,type="l")
points(n0,pod2)
points(n0,pob2)
```  

---  

## Question 9.4  

   Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.  


---  

## Answers  

```{r}
sigma<-c(0.05,0.5,2,16)
dl<-function(x){  ## pdf of standard Laplace
  return(exp(-abs(x))/2)
}
rw.M<-function(sigma,N){   ## generate sample for different sigma
  x0<-rnorm(1,mean=0,sd=sigma)
  x<-numeric(N)
  x[1]<-x0
  u<-runif(N)
  k<-0
  for(i in 2:N){
    y<-rnorm(1,mean=x[i-1],sd=sigma)
    if(u[i]<=dl(y)/dl(x[i-1])) {
      x[i]<-y
      k<-k+1}
    else x[i]<-x[i-1]
  }
  return(list(x=x,k=k))
}

N<-2000
rw1<-rw.M(sigma[1],N)  ## four samples
rw2<-rw.M(sigma[2],N)
rw3<-rw.M(sigma[3],N)
rw4<-rw.M(sigma[4],N)

plot(1:2000,rw1$x,main=expression(sigma==0.05),type="l",xlab=' ',ylab='X')
plot(1:2000,rw2$x,main=expression(sigma==0.5),type="l",xlab=' ',ylab='X')
plot(1:2000,rw3$x,main=expression(sigma==2),type="l",xlab=' ',ylab='X')
plot(1:2000,rw4$x,main=expression(sigma==16),type="l",xlab=' ',ylab='X')
cat('the acceptance rate is',c(rw1$k,rw2$k,rw3$k,rw4$k)/N)
```  

It seems that $\sigma=2$ is suitable.  

---  

## Problem 11.1  

 The natural logarithm and exponential functions are inverses of each other, so that mathematically log(exp x) = exp(log x) = x. Show by example that this property does not hold exactly in computer arithmetic. Does the identity hold with near equality? (See all.equal.)  
 
---  

## Answer  

```{r}
x<-1:100
y<-log(exp(x))
z<-exp(log(x))
print(c(sum((x-y>0)),sum(y-z>0),sum(x-z>0)))
cat(all.equal(x,y),all.equal(y,z),all.equal(x,z))
```  

So this property does not hold exactly in computer arithmetic, but hold with near equality.  

---  

## Problem 11.5  

Write a function to solve the equation  
$$\frac{2\Gamma(\frac{k}{2})}{\sqrt {\pi(k-1)}\Gamma(\frac{k-1}{2})}\displaystyle\int^{c_{k-1}}_0(1+\frac{u^2}{k-1})^{-k/2}du=\frac{2\Gamma(\frac{k+1}{2})}{\sqrt {\pi k}\Gamma(\frac{k}{2})}\displaystyle\int^{c_{k}}_0(1+\frac{u^2}{k})^{-(k+1)/2}du$$  
for a, where  
$$c_k=\sqrt{\frac{a^2k}{k+1-a^2}}.$$  
Compare the solutions with the points A(k) in Exercise 11.4.  

---  

## Answer  

```{r}
A114<-A115<-numeric(25)
k<-c(4:25,100,500,1000)
S<-function(k,x){
  return(1-pt(sqrt(x^2*k/(k+1-x^2)),df=k))
         }
f114<-function(k,x){
  return(S(k,x)-S(k-1,x))
}
for(i in 1:25){
  f<-function(x){
    return(f114(k[i],x))
  }
  b<-sqrt(k[i]-1)
  A114[i]<-uniroot(f,c(0.00001,b-0.00001))$root
}
ck<-function(k,a){
  sqrt(a^2*k/(k+1-a^2))
  }
S2<-function(k,a){

  
  log(2)+lgamma((k+1)/2)-0.5*log(pi*k)-lgamma(k/2)+log(integrate(function(u){(1+u^2/k)^(-(k+1)/2)},0,ck(k,a))$value)-(log(2)+lgamma((k)/2)-0.5*log(pi*(k-1))-lgamma((k-1)/2)+log(integrate(function(u){(1+u^2/(k-1))^(-k/2)},0,ck(k-1,a))$value))

}


for(i in 1:22){
  b<-A114[i]
  A115[i]<-uniroot(S2,interval=c(b-0.1,b+0.1),k=k[i])$root
}
```  

We found that when k is big, our method will produce NA or NAN in 11.5.  

And do the compare for k=4,5,6,...,25:  

```{r}
print(A114[1:22])
print(A115[1:22])
print(A114[1:22]-A115[1:22])
```  

---  

## A-B-O blood type problem  

![](C:\Users\Administrator\Pictures\8.png) 

---  

## Answer 

The iteration is after calculation.  

Write the $E_{\hat p_0,\hat q_0}=a(\hat p_0,\hat q_0)log(p)+b(\hat p_0,\hat q_0)log(q)+c(\hat p_0,\hat q_0)log(1-p-q)+d(\hat p_0,\hat q_0)*log(2)$, and then $\hat p_1 =a/(a+b+c), \hat q_1=b/(a+b+c)$.

```{r}
iter<-function(p,q){
  r<-1-p-q
  a<-2*28*(p^2)/(p^2+2*p*r)+2*p*r/(p^2+2*p*r)*28+70
  b<-2*24*(q^2)/(q^2+2*q*r)+2*q*r/(q^2+2*q*r)*24+70
  c<-2*41+2*p*r/(p^2+2*p*r)*28+2*q*r/(q^2+2*q*r)*24
  pnew<-a/(a+b+c)
  qnew<-b/(a+b+c)

  return(c(pnew,qnew))
}
p<-0.1
q<-0.1
E<-NULL
for(i in 1:10000){
  p<-c(p,iter(p[i],q[i])[1])
  q<-c(q,iter(p[i],q[i])[2])
  if((p[i+1]-p[i]<10^(-5))&(q[i+1]-q[i]<10^(-5)))
    break
}
n<-1:length(p)
cat('the MLE of p,q is',c(p[length(p)],q[length(q)]))
plot(n,p,type="l",col="red")
lines(n,q,col="blue")
```
   
   
---  

## Page 204, exercise 3.  

  Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:  
formulas <- list(  
  mpg ~ disp,  
  mpg ~ I(1 / disp),  
  mpg ~ disp + wt,  
  mpg ~ I(1 / disp) + wt
)  

---  

## Answer 

```{r}
attach(mtcars)
formulas <- list(  
  mpg ~ disp,  
  mpg ~ I(1/disp),  
  mpg ~ disp + wt,  
  mpg ~ I(1 / disp) + wt
)

lapply(formulas,lm,data=mtcars)
for(i in 1:4){
  print(lm(formulas[[i]],data=mtcars))
}
```  

---  

## Page 204, exercise 4.  

  Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply(). Can you do it without an anonymous function?   
bootstraps <- lapply(1:10, function(i) {  
rows <- sample(1:nrow(mtcars), rep = TRUE)  
mtcars[rows, ]  
})

---  

## Answer  

```{r}
bootstraps <- lapply(1:10, function(i) {  
rows <- sample(1:nrow(mtcars), rep = TRUE)  
mtcars[rows, ]  
})
lapply(bootstraps,lm,formula=mpg~disp)
for(i in 1:10){
  print(lm(mpg ~ disp, data = bootstraps[[i]]) )
}
```  

---  


## Page 204, exercise 5.  

5. For each model in the previous two exercises, extract $R^2$ using the function below.  
rsq <- function(mod) summary(mod)$r.squared


---  

## Answer  

```{r}
attach(mtcars)
rsq <- function(mod) summary(mod)$r.squared
formulas <- list(  
  mpg ~ disp,  
  mpg ~ I(1/disp),  
  mpg ~ disp + wt,  
  mpg ~ I(1 / disp) + wt
)
bootstraps <- lapply(1:10, function(i) {  
rows <- sample(1:nrow(mtcars), rep = TRUE)  
mtcars[rows, ]  
})
list1<-lapply(formulas,lm)
list2<-lapply(bootstraps,lm,formula=mpg~disp)
lapply(list1,rsq)
lapply(list2,rsq)
```

---  

## Page 214, exercise 3.  

The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.  
trials <- replicate(100,  
t.test(rpois(10, 10), rpois(7, 10)),  
simplify = FALSE  
)
Extra challenge: get rid of the anonymous function by using [[ directly.  

---  

## Answer  

```{r}
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
sapply(trials, '[[', 3) 
```  


---  

## Page 214, exercise 7.  

Implement mcsapply(), a multicore version of sapply(). Can you implement mcvapply(), a parallel version of vapply()? Why or why not?  

---  

## Answer  


For sapply, just replace 'lapply' in the function with 'mclapply'.  

```{r}

mcsapply<-function(x,f,n){
    f<-match.fun(f)
    answer <- mclapply(x,f,mc.cores=n)
    if (USE.NAMES && is.character(x) && is.null(names(answer))) 
        names(answer) <- x
    if (!isFALSE(simplify) && length(answer)) 
        simplify2array(answer, higher = (simplify == "array"))
    else answer
}
```  

For vapply, we cannot implement mcvapply. Because we have to test that the generated results are properly formatted or not. We don't know how to test them in mc-method. If we can generate the results and test them together in mc-method, then we can implement mcvapply.  


---  

## Exercise  


![](C:\Users\Administrator\Pictures\9.png)  

---  

## Answer  

```{r}
library(Rcpp)
library(microbenchmark)
##Rcpp method
cppFunction('List rw_Mc(double x0, double sigma, int N){
NumericVector x(N);
x[0]=x0;
int k=0;
for(int i=1;i<N;i++){

double y=as<double>(rnorm(1,x[i-1], sigma));
double u=as<double>(runif(1));

if (u<=exp(-abs(y))/exp(-abs(x[i-1]))) {
x[i]=y;
k=k+1;  
}
else x[i]=x[i-1];
}

return(List::create(Named("x")=x,Named("k")=k));
}')
## R method
rw.M<-function(x0,sigma,N){   ## generate sample for different sigma
  x<-numeric(N)
  x[1]<-x0
  u<-runif(N)
  k<-0
  for(i in 2:N){
    y<-rnorm(1,mean=x[i-1],sd=sigma)
    if(u[i]<=exp(-abs(y))/exp(-abs(x[i-1]))) {
      x[i]<-y
      k<-k+1}
    else x[i]<-x[i-1]
  }
  return(list(x=x,k=k))
}
sigma<-c(0.05,0.5,2,16)
x0<-25
N<-2000
rw1<-rw.M(x0,sigma[1],N)  ## four samples
rw2<-rw.M(x0,sigma[2],N)
rw3<-rw.M(x0,sigma[3],N)
rw4<-rw.M(x0,sigma[4],N)
rwC1<-rw_Mc(x0,sigma[1],N)  ## four samples
rwC2<-rw_Mc(x0,sigma[2],N)
rwC3<-rw_Mc(x0,sigma[3],N)
rwC4<-rw_Mc(x0,sigma[4],N)

##comparison
qqplot(rw1$x,rwC1$x)
qqplot(rw2$x,rwC2$x)
qqplot(rw3$x,rwC4$x)
qqplot(rw4$x,rwC4$x)
##computation time
ts1<-microbenchmark(rw.M(x0,sigma[1],N),rw_Mc(x0,sigma[1],N))
ts2<-microbenchmark(rw.M(x0,sigma[2],N),rw_Mc(x0,sigma[2],N))
ts3<-microbenchmark(rw.M(x0,sigma[3],N),rw_Mc(x0,sigma[3],N))
ts4<-microbenchmark(rw.M(x0,sigma[4],N),rw_Mc(x0,sigma[4],N))
summary(ts1)[,c(1,3,5,6)]
summary(ts2)[,c(1,3,5,6)]
summary(ts3)[,c(1,3,5,6)]
summary(ts4)[,c(1,3,5,6)]
```  

Hence, we found that Rcpp save much time. The qqplots showed that they are not totally the same, which may be due to the low number of Rcpp acceptances.  


